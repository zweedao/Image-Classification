{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cornell Deep Learning Homework 2 - CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "94d6bb311a3a4f838414eb321b83fbd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a3438c2aa769410894edde94b938604a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9f8a749990b64d77875cf8dfc8f3a516",
              "IPY_MODEL_f97fdc98807e4869b51364e7df6398f2"
            ]
          }
        },
        "a3438c2aa769410894edde94b938604a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9f8a749990b64d77875cf8dfc8f3a516": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b219652916374d64bc9767b22df0e015",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ec98d1e35509498b82438bbbe0d9daab"
          }
        },
        "f97fdc98807e4869b51364e7df6398f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d4dfb9153abc4ddaa1647daf94f12117",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 86466093.98it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0eb4c2ac2fca414da77c09ed3ed3df29"
          }
        },
        "b219652916374d64bc9767b22df0e015": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ec98d1e35509498b82438bbbe0d9daab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4dfb9153abc4ddaa1647daf94f12117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0eb4c2ac2fca414da77c09ed3ed3df29": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_REaRrP5aVw",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWUKa_FMwTRl",
        "colab_type": "code",
        "outputId": "d344dfba-9dd3-4e93-9772-a953b1d9459b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "94d6bb311a3a4f838414eb321b83fbd0",
            "a3438c2aa769410894edde94b938604a",
            "9f8a749990b64d77875cf8dfc8f3a516",
            "f97fdc98807e4869b51364e7df6398f2",
            "b219652916374d64bc9767b22df0e015",
            "ec98d1e35509498b82438bbbe0d9daab",
            "d4dfb9153abc4ddaa1647daf94f12117",
            "0eb4c2ac2fca414da77c09ed3ed3df29"
          ]
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import argparse\n",
        "import csv\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as img\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "import torch.utils.data as data_utils\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, AvgPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "\n",
        "\n",
        "# Problem 3 - Training a small CNN\n",
        "#Parameters\n",
        "cuda_enabled = True\n",
        "batch_size = 5000\n",
        "\n",
        "# convert data to a normalized torch.FloatTensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "# choose the training and test datasets\n",
        "train_data = datasets.CIFAR10('data', train=True,\n",
        "                              download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10('data', train=False,\n",
        "                             download=True, transform=transform)\n",
        "\n",
        "# obtain training indices that will be used for validation\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "np.random.shuffle(indices)\n",
        "split = int(np.floor(0.2 * num_train))\n",
        "train_idx, dev_idx = indices[split:], indices[:split]\n",
        "\n",
        "# define samplers for obtaining training and validation batches\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "dev_sampler = SubsetRandomSampler(dev_idx)\n",
        "\n",
        "# prepare data loaders (combine dataset and sampler)\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "    sampler=train_sampler)\n",
        "dev_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, \n",
        "    sampler=dev_sampler)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "# specify the image classes\n",
        "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "# print one batch of training images\n",
        "# dataiter = iter(train_loader)\n",
        "# images, labels = dataiter.next()\n",
        "# images = images.numpy() # convert images to numpy for display\n",
        "\n",
        "# for i in np.arange(len(images)):\n",
        "#     img = images[i] / 2 + 0.5  # unnormalize\n",
        "#     img = np.transpose(img, (1, 2, 0))\n",
        "#     plt.imshow(img)\n",
        "#     plt.title(classes[labels[i]])\n",
        "#     plt.show()\n",
        "\n",
        "\n",
        "# define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # convolutional layer\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=11, padding=1)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
        "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=3)\n",
        "        self.fc = nn.Linear(128*4*4, 10)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        # add sequence of convolutional and max pooling layers\n",
        "        #print('x',x.shape)\n",
        "        x = self.conv1(x)\n",
        "        #print('conv1',x.shape)\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.maxpool(x)\n",
        "        #print('maxpool',x.shape)  \n",
        "        x = self.conv2(x)\n",
        "        #print('conv2',x.shape)\n",
        "        x = self.batch_norm2(x)\n",
        "        x = self.conv3(x)\n",
        "        #print('conv3',x.shape)\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.relu(x)\n",
        "        #print('relu',x.shape)\n",
        "        x = self.avgpool(x)\n",
        "        #print('avgpool',x.shape)\n",
        "\n",
        "        # flattening\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        #print('x.view',x.shape)\n",
        "\n",
        "        # fully connected layers\n",
        "        x = self.fc(x)\n",
        "        #print('fc',x.shape)\n",
        "        x = self.softmax(x)\n",
        "        #print('softmax',x.shape)\n",
        "        return x\n",
        "    \n",
        "\n",
        "model = CNN()\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "if cuda_enabled:\n",
        "    model = model.cuda()\n",
        "    loss_function = loss_function.cuda()\n",
        "\n",
        "print(model)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=0) \n",
        "epochs = 100\n",
        "\n",
        "loss_history_train = []\n",
        "loss_history_dev = []\n",
        "\n",
        "for i in range(epochs):\n",
        "    loss_total_train = 0\n",
        "    loss_total_dev = 0\n",
        "\n",
        "    #Train set\n",
        "    model.train()\n",
        "    j = 0\n",
        "    for input, target in train_loader:\n",
        "        print('epoch',i,'train batch',j)\n",
        "        if cuda_enabled:\n",
        "            input, target = input.cuda(), target.cuda()\n",
        "        # Forward Propagation\n",
        "        output = model(input)\n",
        "        # Compute and print loss\n",
        "        loss_train = loss_function(output, target)\n",
        "        loss_total_train += loss_train.item()\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()   \n",
        "        # perform a backward pass (backpropagation)\n",
        "        loss_train.backward()\n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "        j +=1\n",
        "        \n",
        "    lose_avg_train = float(loss_total_train)/j \n",
        "    loss_history_train.append(lose_avg_train)\n",
        "\n",
        "    #Dev set\n",
        "    model.eval()\n",
        "    j = 0\n",
        "    for input, target in dev_loader:\n",
        "        print('epoch',i,'dev batch',j)\n",
        "        if cuda_enabled:\n",
        "            input, target = input.cuda(), target.cuda()\n",
        "        # Forward Propagation\n",
        "        output = model(input)\n",
        "        # Compute and print loss\n",
        "        loss_dev = loss_function(output, target)\n",
        "        loss_total_dev += loss_dev.item()\n",
        "        j +=1\n",
        "        \n",
        "    lose_avg_dev = float(loss_total_dev)/j \n",
        "    loss_history_dev.append(lose_avg_dev)\n",
        "\n",
        "    print('epoch:',i,'loss train:', lose_avg_train, 'loss dev:', lose_avg_dev)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94d6bb311a3a4f838414eb321b83fbd0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n",
            "CNN(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(11, 11), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (batch_norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (avgpool): AvgPool2d(kernel_size=3, stride=3, padding=0)\n",
            "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n",
            "epoch 0 train batch 0\n",
            "epoch 0 train batch 1\n",
            "epoch 0 train batch 2\n",
            "epoch 0 train batch 3\n",
            "epoch 0 train batch 4\n",
            "epoch 0 train batch 5\n",
            "epoch 0 train batch 6\n",
            "epoch 0 train batch 7\n",
            "epoch 0 dev batch 0\n",
            "epoch 0 dev batch 1\n",
            "epoch: 0 loss train: 2.260271340608597 loss dev: 2.2600018978118896\n",
            "epoch 1 train batch 0\n",
            "epoch 1 train batch 1\n",
            "epoch 1 train batch 2\n",
            "epoch 1 train batch 3\n",
            "epoch 1 train batch 4\n",
            "epoch 1 train batch 5\n",
            "epoch 1 train batch 6\n",
            "epoch 1 train batch 7\n",
            "epoch 1 dev batch 0\n",
            "epoch 1 dev batch 1\n",
            "epoch: 1 loss train: 2.151115894317627 loss dev: 2.1600260734558105\n",
            "epoch 2 train batch 0\n",
            "epoch 2 train batch 1\n",
            "epoch 2 train batch 2\n",
            "epoch 2 train batch 3\n",
            "epoch 2 train batch 4\n",
            "epoch 2 train batch 5\n",
            "epoch 2 train batch 6\n",
            "epoch 2 train batch 7\n",
            "epoch 2 dev batch 0\n",
            "epoch 2 dev batch 1\n",
            "epoch: 2 loss train: 2.094685912132263 loss dev: 2.0973705053329468\n",
            "epoch 3 train batch 0\n",
            "epoch 3 train batch 1\n",
            "epoch 3 train batch 2\n",
            "epoch 3 train batch 3\n",
            "epoch 3 train batch 4\n",
            "epoch 3 train batch 5\n",
            "epoch 3 train batch 6\n",
            "epoch 3 train batch 7\n",
            "epoch 3 dev batch 0\n",
            "epoch 3 dev batch 1\n",
            "epoch: 3 loss train: 2.0691291987895966 loss dev: 2.0588622093200684\n",
            "epoch 4 train batch 0\n",
            "epoch 4 train batch 1\n",
            "epoch 4 train batch 2\n",
            "epoch 4 train batch 3\n",
            "epoch 4 train batch 4\n",
            "epoch 4 train batch 5\n",
            "epoch 4 train batch 6\n",
            "epoch 4 train batch 7\n",
            "epoch 4 dev batch 0\n",
            "epoch 4 dev batch 1\n",
            "epoch: 4 loss train: 2.045554995536804 loss dev: 2.0578113794326782\n",
            "epoch 5 train batch 0\n",
            "epoch 5 train batch 1\n",
            "epoch 5 train batch 2\n",
            "epoch 5 train batch 3\n",
            "epoch 5 train batch 4\n",
            "epoch 5 train batch 5\n",
            "epoch 5 train batch 6\n",
            "epoch 5 train batch 7\n",
            "epoch 5 dev batch 0\n",
            "epoch 5 dev batch 1\n",
            "epoch: 5 loss train: 2.0284957587718964 loss dev: 2.0293338298797607\n",
            "epoch 6 train batch 0\n",
            "epoch 6 train batch 1\n",
            "epoch 6 train batch 2\n",
            "epoch 6 train batch 3\n",
            "epoch 6 train batch 4\n",
            "epoch 6 train batch 5\n",
            "epoch 6 train batch 6\n",
            "epoch 6 train batch 7\n",
            "epoch 6 dev batch 0\n",
            "epoch 6 dev batch 1\n",
            "epoch: 6 loss train: 2.0108955800533295 loss dev: 2.015274167060852\n",
            "epoch 7 train batch 0\n",
            "epoch 7 train batch 1\n",
            "epoch 7 train batch 2\n",
            "epoch 7 train batch 3\n",
            "epoch 7 train batch 4\n",
            "epoch 7 train batch 5\n",
            "epoch 7 train batch 6\n",
            "epoch 7 train batch 7\n",
            "epoch 7 dev batch 0\n",
            "epoch 7 dev batch 1\n",
            "epoch: 7 loss train: 1.9960425943136215 loss dev: 2.0379687547683716\n",
            "epoch 8 train batch 0\n",
            "epoch 8 train batch 1\n",
            "epoch 8 train batch 2\n",
            "epoch 8 train batch 3\n",
            "epoch 8 train batch 4\n",
            "epoch 8 train batch 5\n",
            "epoch 8 train batch 6\n",
            "epoch 8 train batch 7\n",
            "epoch 8 dev batch 0\n",
            "epoch 8 dev batch 1\n",
            "epoch: 8 loss train: 1.9912802577018738 loss dev: 1.9885892868041992\n",
            "epoch 9 train batch 0\n",
            "epoch 9 train batch 1\n",
            "epoch 9 train batch 2\n",
            "epoch 9 train batch 3\n",
            "epoch 9 train batch 4\n",
            "epoch 9 train batch 5\n",
            "epoch 9 train batch 6\n",
            "epoch 9 train batch 7\n",
            "epoch 9 dev batch 0\n",
            "epoch 9 dev batch 1\n",
            "epoch: 9 loss train: 1.9724600464105606 loss dev: 2.0175743103027344\n",
            "epoch 10 train batch 0\n",
            "epoch 10 train batch 1\n",
            "epoch 10 train batch 2\n",
            "epoch 10 train batch 3\n",
            "epoch 10 train batch 4\n",
            "epoch 10 train batch 5\n",
            "epoch 10 train batch 6\n",
            "epoch 10 train batch 7\n",
            "epoch 10 dev batch 0\n",
            "epoch 10 dev batch 1\n",
            "epoch: 10 loss train: 1.966568723320961 loss dev: 1.9940385222434998\n",
            "epoch 11 train batch 0\n",
            "epoch 11 train batch 1\n",
            "epoch 11 train batch 2\n",
            "epoch 11 train batch 3\n",
            "epoch 11 train batch 4\n",
            "epoch 11 train batch 5\n",
            "epoch 11 train batch 6\n",
            "epoch 11 train batch 7\n",
            "epoch 11 dev batch 0\n",
            "epoch 11 dev batch 1\n",
            "epoch: 11 loss train: 1.9535400420427322 loss dev: 1.98328697681427\n",
            "epoch 12 train batch 0\n",
            "epoch 12 train batch 1\n",
            "epoch 12 train batch 2\n",
            "epoch 12 train batch 3\n",
            "epoch 12 train batch 4\n",
            "epoch 12 train batch 5\n",
            "epoch 12 train batch 6\n",
            "epoch 12 train batch 7\n",
            "epoch 12 dev batch 0\n",
            "epoch 12 dev batch 1\n",
            "epoch: 12 loss train: 1.9430487006902695 loss dev: 1.9610660672187805\n",
            "epoch 13 train batch 0\n",
            "epoch 13 train batch 1\n",
            "epoch 13 train batch 2\n",
            "epoch 13 train batch 3\n",
            "epoch 13 train batch 4\n",
            "epoch 13 train batch 5\n",
            "epoch 13 train batch 6\n",
            "epoch 13 train batch 7\n",
            "epoch 13 dev batch 0\n",
            "epoch 13 dev batch 1\n",
            "epoch: 13 loss train: 1.939761444926262 loss dev: 2.009464740753174\n",
            "epoch 14 train batch 0\n",
            "epoch 14 train batch 1\n",
            "epoch 14 train batch 2\n",
            "epoch 14 train batch 3\n",
            "epoch 14 train batch 4\n",
            "epoch 14 train batch 5\n",
            "epoch 14 train batch 6\n",
            "epoch 14 train batch 7\n",
            "epoch 14 dev batch 0\n",
            "epoch 14 dev batch 1\n",
            "epoch: 14 loss train: 1.9334652423858643 loss dev: 1.9281529784202576\n",
            "epoch 15 train batch 0\n",
            "epoch 15 train batch 1\n",
            "epoch 15 train batch 2\n",
            "epoch 15 train batch 3\n",
            "epoch 15 train batch 4\n",
            "epoch 15 train batch 5\n",
            "epoch 15 train batch 6\n",
            "epoch 15 train batch 7\n",
            "epoch 15 dev batch 0\n",
            "epoch 15 dev batch 1\n",
            "epoch: 15 loss train: 1.9159782528877258 loss dev: 1.9391010403633118\n",
            "epoch 16 train batch 0\n",
            "epoch 16 train batch 1\n",
            "epoch 16 train batch 2\n",
            "epoch 16 train batch 3\n",
            "epoch 16 train batch 4\n",
            "epoch 16 train batch 5\n",
            "epoch 16 train batch 6\n",
            "epoch 16 train batch 7\n",
            "epoch 16 dev batch 0\n",
            "epoch 16 dev batch 1\n",
            "epoch: 16 loss train: 1.904536023736 loss dev: 1.9276562333106995\n",
            "epoch 17 train batch 0\n",
            "epoch 17 train batch 1\n",
            "epoch 17 train batch 2\n",
            "epoch 17 train batch 3\n",
            "epoch 17 train batch 4\n",
            "epoch 17 train batch 5\n",
            "epoch 17 train batch 6\n",
            "epoch 17 train batch 7\n",
            "epoch 17 dev batch 0\n",
            "epoch 17 dev batch 1\n",
            "epoch: 17 loss train: 1.9009130746126175 loss dev: 1.910635530948639\n",
            "epoch 18 train batch 0\n",
            "epoch 18 train batch 1\n",
            "epoch 18 train batch 2\n",
            "epoch 18 train batch 3\n",
            "epoch 18 train batch 4\n",
            "epoch 18 train batch 5\n",
            "epoch 18 train batch 6\n",
            "epoch 18 train batch 7\n",
            "epoch 18 dev batch 0\n",
            "epoch 18 dev batch 1\n",
            "epoch: 18 loss train: 1.890760913491249 loss dev: 1.9195538759231567\n",
            "epoch 19 train batch 0\n",
            "epoch 19 train batch 1\n",
            "epoch 19 train batch 2\n",
            "epoch 19 train batch 3\n",
            "epoch 19 train batch 4\n",
            "epoch 19 train batch 5\n",
            "epoch 19 train batch 6\n",
            "epoch 19 train batch 7\n",
            "epoch 19 dev batch 0\n",
            "epoch 19 dev batch 1\n",
            "epoch: 19 loss train: 1.882775142788887 loss dev: 1.9264093041419983\n",
            "epoch 20 train batch 0\n",
            "epoch 20 train batch 1\n",
            "epoch 20 train batch 2\n",
            "epoch 20 train batch 3\n",
            "epoch 20 train batch 4\n",
            "epoch 20 train batch 5\n",
            "epoch 20 train batch 6\n",
            "epoch 20 train batch 7\n",
            "epoch 20 dev batch 0\n",
            "epoch 20 dev batch 1\n",
            "epoch: 20 loss train: 1.876759022474289 loss dev: 1.9014248847961426\n",
            "epoch 21 train batch 0\n",
            "epoch 21 train batch 1\n",
            "epoch 21 train batch 2\n",
            "epoch 21 train batch 3\n",
            "epoch 21 train batch 4\n",
            "epoch 21 train batch 5\n",
            "epoch 21 train batch 6\n",
            "epoch 21 train batch 7\n",
            "epoch 21 dev batch 0\n",
            "epoch 21 dev batch 1\n",
            "epoch: 21 loss train: 1.881410300731659 loss dev: 2.040113687515259\n",
            "epoch 22 train batch 0\n",
            "epoch 22 train batch 1\n",
            "epoch 22 train batch 2\n",
            "epoch 22 train batch 3\n",
            "epoch 22 train batch 4\n",
            "epoch 22 train batch 5\n",
            "epoch 22 train batch 6\n",
            "epoch 22 train batch 7\n",
            "epoch 22 dev batch 0\n",
            "epoch 22 dev batch 1\n",
            "epoch: 22 loss train: 1.8787762969732285 loss dev: 1.9709400534629822\n",
            "epoch 23 train batch 0\n",
            "epoch 23 train batch 1\n",
            "epoch 23 train batch 2\n",
            "epoch 23 train batch 3\n",
            "epoch 23 train batch 4\n",
            "epoch 23 train batch 5\n",
            "epoch 23 train batch 6\n",
            "epoch 23 train batch 7\n",
            "epoch 23 dev batch 0\n",
            "epoch 23 dev batch 1\n",
            "epoch: 23 loss train: 1.8697785437107086 loss dev: 1.8911386728286743\n",
            "epoch 24 train batch 0\n",
            "epoch 24 train batch 1\n",
            "epoch 24 train batch 2\n",
            "epoch 24 train batch 3\n",
            "epoch 24 train batch 4\n",
            "epoch 24 train batch 5\n",
            "epoch 24 train batch 6\n",
            "epoch 24 train batch 7\n",
            "epoch 24 dev batch 0\n",
            "epoch 24 dev batch 1\n",
            "epoch: 24 loss train: 1.853969618678093 loss dev: 1.893479585647583\n",
            "epoch 25 train batch 0\n",
            "epoch 25 train batch 1\n",
            "epoch 25 train batch 2\n",
            "epoch 25 train batch 3\n",
            "epoch 25 train batch 4\n",
            "epoch 25 train batch 5\n",
            "epoch 25 train batch 6\n",
            "epoch 25 train batch 7\n",
            "epoch 25 dev batch 0\n",
            "epoch 25 dev batch 1\n",
            "epoch: 25 loss train: 1.855987474322319 loss dev: 1.889565348625183\n",
            "epoch 26 train batch 0\n",
            "epoch 26 train batch 1\n",
            "epoch 26 train batch 2\n",
            "epoch 26 train batch 3\n",
            "epoch 26 train batch 4\n",
            "epoch 26 train batch 5\n",
            "epoch 26 train batch 6\n",
            "epoch 26 train batch 7\n",
            "epoch 26 dev batch 0\n",
            "epoch 26 dev batch 1\n",
            "epoch: 26 loss train: 1.8482116162776947 loss dev: 1.8923354744911194\n",
            "epoch 27 train batch 0\n",
            "epoch 27 train batch 1\n",
            "epoch 27 train batch 2\n",
            "epoch 27 train batch 3\n",
            "epoch 27 train batch 4\n",
            "epoch 27 train batch 5\n",
            "epoch 27 train batch 6\n",
            "epoch 27 train batch 7\n",
            "epoch 27 dev batch 0\n",
            "epoch 27 dev batch 1\n",
            "epoch: 27 loss train: 1.8393349945545197 loss dev: 1.8701277375221252\n",
            "epoch 28 train batch 0\n",
            "epoch 28 train batch 1\n",
            "epoch 28 train batch 2\n",
            "epoch 28 train batch 3\n",
            "epoch 28 train batch 4\n",
            "epoch 28 train batch 5\n",
            "epoch 28 train batch 6\n",
            "epoch 28 train batch 7\n",
            "epoch 28 dev batch 0\n",
            "epoch 28 dev batch 1\n",
            "epoch: 28 loss train: 1.8346926867961884 loss dev: 1.8924967050552368\n",
            "epoch 29 train batch 0\n",
            "epoch 29 train batch 1\n",
            "epoch 29 train batch 2\n",
            "epoch 29 train batch 3\n",
            "epoch 29 train batch 4\n",
            "epoch 29 train batch 5\n",
            "epoch 29 train batch 6\n",
            "epoch 29 train batch 7\n",
            "epoch 29 dev batch 0\n",
            "epoch 29 dev batch 1\n",
            "epoch: 29 loss train: 1.8372208625078201 loss dev: 1.8555755615234375\n",
            "epoch 30 train batch 0\n",
            "epoch 30 train batch 1\n",
            "epoch 30 train batch 2\n",
            "epoch 30 train batch 3\n",
            "epoch 30 train batch 4\n",
            "epoch 30 train batch 5\n",
            "epoch 30 train batch 6\n",
            "epoch 30 train batch 7\n",
            "epoch 30 dev batch 0\n",
            "epoch 30 dev batch 1\n",
            "epoch: 30 loss train: 1.8262769132852554 loss dev: 1.8525025248527527\n",
            "epoch 31 train batch 0\n",
            "epoch 31 train batch 1\n",
            "epoch 31 train batch 2\n",
            "epoch 31 train batch 3\n",
            "epoch 31 train batch 4\n",
            "epoch 31 train batch 5\n",
            "epoch 31 train batch 6\n",
            "epoch 31 train batch 7\n",
            "epoch 31 dev batch 0\n",
            "epoch 31 dev batch 1\n",
            "epoch: 31 loss train: 1.8216041177511215 loss dev: 1.8499345779418945\n",
            "epoch 32 train batch 0\n",
            "epoch 32 train batch 1\n",
            "epoch 32 train batch 2\n",
            "epoch 32 train batch 3\n",
            "epoch 32 train batch 4\n",
            "epoch 32 train batch 5\n",
            "epoch 32 train batch 6\n",
            "epoch 32 train batch 7\n",
            "epoch 32 dev batch 0\n",
            "epoch 32 dev batch 1\n",
            "epoch: 32 loss train: 1.8187141865491867 loss dev: 1.8521207571029663\n",
            "epoch 33 train batch 0\n",
            "epoch 33 train batch 1\n",
            "epoch 33 train batch 2\n",
            "epoch 33 train batch 3\n",
            "epoch 33 train batch 4\n",
            "epoch 33 train batch 5\n",
            "epoch 33 train batch 6\n",
            "epoch 33 train batch 7\n",
            "epoch 33 dev batch 0\n",
            "epoch 33 dev batch 1\n",
            "epoch: 33 loss train: 1.8146177530288696 loss dev: 1.8888052701950073\n",
            "epoch 34 train batch 0\n",
            "epoch 34 train batch 1\n",
            "epoch 34 train batch 2\n",
            "epoch 34 train batch 3\n",
            "epoch 34 train batch 4\n",
            "epoch 34 train batch 5\n",
            "epoch 34 train batch 6\n",
            "epoch 34 train batch 7\n",
            "epoch 34 dev batch 0\n",
            "epoch 34 dev batch 1\n",
            "epoch: 34 loss train: 1.8204623460769653 loss dev: 1.8646045327186584\n",
            "epoch 35 train batch 0\n",
            "epoch 35 train batch 1\n",
            "epoch 35 train batch 2\n",
            "epoch 35 train batch 3\n",
            "epoch 35 train batch 4\n",
            "epoch 35 train batch 5\n",
            "epoch 35 train batch 6\n",
            "epoch 35 train batch 7\n",
            "epoch 35 dev batch 0\n",
            "epoch 35 dev batch 1\n",
            "epoch: 35 loss train: 1.816881537437439 loss dev: 1.8493361473083496\n",
            "epoch 36 train batch 0\n",
            "epoch 36 train batch 1\n",
            "epoch 36 train batch 2\n",
            "epoch 36 train batch 3\n",
            "epoch 36 train batch 4\n",
            "epoch 36 train batch 5\n",
            "epoch 36 train batch 6\n",
            "epoch 36 train batch 7\n",
            "epoch 36 dev batch 0\n",
            "epoch 36 dev batch 1\n",
            "epoch: 36 loss train: 1.8107148110866547 loss dev: 1.8307639956474304\n",
            "epoch 37 train batch 0\n",
            "epoch 37 train batch 1\n",
            "epoch 37 train batch 2\n",
            "epoch 37 train batch 3\n",
            "epoch 37 train batch 4\n",
            "epoch 37 train batch 5\n",
            "epoch 37 train batch 6\n",
            "epoch 37 train batch 7\n",
            "epoch 37 dev batch 0\n",
            "epoch 37 dev batch 1\n",
            "epoch: 37 loss train: 1.8022887259721756 loss dev: 1.8254938125610352\n",
            "epoch 38 train batch 0\n",
            "epoch 38 train batch 1\n",
            "epoch 38 train batch 2\n",
            "epoch 38 train batch 3\n",
            "epoch 38 train batch 4\n",
            "epoch 38 train batch 5\n",
            "epoch 38 train batch 6\n",
            "epoch 38 train batch 7\n",
            "epoch 38 dev batch 0\n",
            "epoch 38 dev batch 1\n",
            "epoch: 38 loss train: 1.80112424492836 loss dev: 1.8359090089797974\n",
            "epoch 39 train batch 0\n",
            "epoch 39 train batch 1\n",
            "epoch 39 train batch 2\n",
            "epoch 39 train batch 3\n",
            "epoch 39 train batch 4\n",
            "epoch 39 train batch 5\n",
            "epoch 39 train batch 6\n",
            "epoch 39 train batch 7\n",
            "epoch 39 dev batch 0\n",
            "epoch 39 dev batch 1\n",
            "epoch: 39 loss train: 1.7962719202041626 loss dev: 1.8358237147331238\n",
            "epoch 40 train batch 0\n",
            "epoch 40 train batch 1\n",
            "epoch 40 train batch 2\n",
            "epoch 40 train batch 3\n",
            "epoch 40 train batch 4\n",
            "epoch 40 train batch 5\n",
            "epoch 40 train batch 6\n",
            "epoch 40 train batch 7\n",
            "epoch 40 dev batch 0\n",
            "epoch 40 dev batch 1\n",
            "epoch: 40 loss train: 1.7968056052923203 loss dev: 1.8171559572219849\n",
            "epoch 41 train batch 0\n",
            "epoch 41 train batch 1\n",
            "epoch 41 train batch 2\n",
            "epoch 41 train batch 3\n",
            "epoch 41 train batch 4\n",
            "epoch 41 train batch 5\n",
            "epoch 41 train batch 6\n",
            "epoch 41 train batch 7\n",
            "epoch 41 dev batch 0\n",
            "epoch 41 dev batch 1\n",
            "epoch: 41 loss train: 1.7927077561616898 loss dev: 1.8569470643997192\n",
            "epoch 42 train batch 0\n",
            "epoch 42 train batch 1\n",
            "epoch 42 train batch 2\n",
            "epoch 42 train batch 3\n",
            "epoch 42 train batch 4\n",
            "epoch 42 train batch 5\n",
            "epoch 42 train batch 6\n",
            "epoch 42 train batch 7\n",
            "epoch 42 dev batch 0\n",
            "epoch 42 dev batch 1\n",
            "epoch: 42 loss train: 1.7912870049476624 loss dev: 1.8361176252365112\n",
            "epoch 43 train batch 0\n",
            "epoch 43 train batch 1\n",
            "epoch 43 train batch 2\n",
            "epoch 43 train batch 3\n",
            "epoch 43 train batch 4\n",
            "epoch 43 train batch 5\n",
            "epoch 43 train batch 6\n",
            "epoch 43 train batch 7\n",
            "epoch 43 dev batch 0\n",
            "epoch 43 dev batch 1\n",
            "epoch: 43 loss train: 1.7873542755842209 loss dev: 1.8076876401901245\n",
            "epoch 44 train batch 0\n",
            "epoch 44 train batch 1\n",
            "epoch 44 train batch 2\n",
            "epoch 44 train batch 3\n",
            "epoch 44 train batch 4\n",
            "epoch 44 train batch 5\n",
            "epoch 44 train batch 6\n",
            "epoch 44 train batch 7\n",
            "epoch 44 dev batch 0\n",
            "epoch 44 dev batch 1\n",
            "epoch: 44 loss train: 1.7797187715768814 loss dev: 1.8218764662742615\n",
            "epoch 45 train batch 0\n",
            "epoch 45 train batch 1\n",
            "epoch 45 train batch 2\n",
            "epoch 45 train batch 3\n",
            "epoch 45 train batch 4\n",
            "epoch 45 train batch 5\n",
            "epoch 45 train batch 6\n",
            "epoch 45 train batch 7\n",
            "epoch 45 dev batch 0\n",
            "epoch 45 dev batch 1\n",
            "epoch: 45 loss train: 1.7788694649934769 loss dev: 1.8321022391319275\n",
            "epoch 46 train batch 0\n",
            "epoch 46 train batch 1\n",
            "epoch 46 train batch 2\n",
            "epoch 46 train batch 3\n",
            "epoch 46 train batch 4\n",
            "epoch 46 train batch 5\n",
            "epoch 46 train batch 6\n",
            "epoch 46 train batch 7\n",
            "epoch 46 dev batch 0\n",
            "epoch 46 dev batch 1\n",
            "epoch: 46 loss train: 1.7851404696702957 loss dev: 1.8423640727996826\n",
            "epoch 47 train batch 0\n",
            "epoch 47 train batch 1\n",
            "epoch 47 train batch 2\n",
            "epoch 47 train batch 3\n",
            "epoch 47 train batch 4\n",
            "epoch 47 train batch 5\n",
            "epoch 47 train batch 6\n",
            "epoch 47 train batch 7\n",
            "epoch 47 dev batch 0\n",
            "epoch 47 dev batch 1\n",
            "epoch: 47 loss train: 1.7778141647577286 loss dev: 1.8108744621276855\n",
            "epoch 48 train batch 0\n",
            "epoch 48 train batch 1\n",
            "epoch 48 train batch 2\n",
            "epoch 48 train batch 3\n",
            "epoch 48 train batch 4\n",
            "epoch 48 train batch 5\n",
            "epoch 48 train batch 6\n",
            "epoch 48 train batch 7\n",
            "epoch 48 dev batch 0\n",
            "epoch 48 dev batch 1\n",
            "epoch: 48 loss train: 1.7729769051074982 loss dev: 1.8203606605529785\n",
            "epoch 49 train batch 0\n",
            "epoch 49 train batch 1\n",
            "epoch 49 train batch 2\n",
            "epoch 49 train batch 3\n",
            "epoch 49 train batch 4\n",
            "epoch 49 train batch 5\n",
            "epoch 49 train batch 6\n",
            "epoch 49 train batch 7\n",
            "epoch 49 dev batch 0\n",
            "epoch 49 dev batch 1\n",
            "epoch: 49 loss train: 1.7689876109361649 loss dev: 1.818127691745758\n",
            "epoch 50 train batch 0\n",
            "epoch 50 train batch 1\n",
            "epoch 50 train batch 2\n",
            "epoch 50 train batch 3\n",
            "epoch 50 train batch 4\n",
            "epoch 50 train batch 5\n",
            "epoch 50 train batch 6\n",
            "epoch 50 train batch 7\n",
            "epoch 50 dev batch 0\n",
            "epoch 50 dev batch 1\n",
            "epoch: 50 loss train: 1.7709286659955978 loss dev: 1.8328051567077637\n",
            "epoch 51 train batch 0\n",
            "epoch 51 train batch 1\n",
            "epoch 51 train batch 2\n",
            "epoch 51 train batch 3\n",
            "epoch 51 train batch 4\n",
            "epoch 51 train batch 5\n",
            "epoch 51 train batch 6\n",
            "epoch 51 train batch 7\n",
            "epoch 51 dev batch 0\n",
            "epoch 51 dev batch 1\n",
            "epoch: 51 loss train: 1.765926793217659 loss dev: 1.792863368988037\n",
            "epoch 52 train batch 0\n",
            "epoch 52 train batch 1\n",
            "epoch 52 train batch 2\n",
            "epoch 52 train batch 3\n",
            "epoch 52 train batch 4\n",
            "epoch 52 train batch 5\n",
            "epoch 52 train batch 6\n",
            "epoch 52 train batch 7\n",
            "epoch 52 dev batch 0\n",
            "epoch 52 dev batch 1\n",
            "epoch: 52 loss train: 1.7612550556659698 loss dev: 1.791747808456421\n",
            "epoch 53 train batch 0\n",
            "epoch 53 train batch 1\n",
            "epoch 53 train batch 2\n",
            "epoch 53 train batch 3\n",
            "epoch 53 train batch 4\n",
            "epoch 53 train batch 5\n",
            "epoch 53 train batch 6\n",
            "epoch 53 train batch 7\n",
            "epoch 53 dev batch 0\n",
            "epoch 53 dev batch 1\n",
            "epoch: 53 loss train: 1.758899986743927 loss dev: 1.8020268678665161\n",
            "epoch 54 train batch 0\n",
            "epoch 54 train batch 1\n",
            "epoch 54 train batch 2\n",
            "epoch 54 train batch 3\n",
            "epoch 54 train batch 4\n",
            "epoch 54 train batch 5\n",
            "epoch 54 train batch 6\n",
            "epoch 54 train batch 7\n",
            "epoch 54 dev batch 0\n",
            "epoch 54 dev batch 1\n",
            "epoch: 54 loss train: 1.7608390003442764 loss dev: 1.806355357170105\n",
            "epoch 55 train batch 0\n",
            "epoch 55 train batch 1\n",
            "epoch 55 train batch 2\n",
            "epoch 55 train batch 3\n",
            "epoch 55 train batch 4\n",
            "epoch 55 train batch 5\n",
            "epoch 55 train batch 6\n",
            "epoch 55 train batch 7\n",
            "epoch 55 dev batch 0\n",
            "epoch 55 dev batch 1\n",
            "epoch: 55 loss train: 1.7595357447862625 loss dev: 1.8230109214782715\n",
            "epoch 56 train batch 0\n",
            "epoch 56 train batch 1\n",
            "epoch 56 train batch 2\n",
            "epoch 56 train batch 3\n",
            "epoch 56 train batch 4\n",
            "epoch 56 train batch 5\n",
            "epoch 56 train batch 6\n",
            "epoch 56 train batch 7\n",
            "epoch 56 dev batch 0\n",
            "epoch 56 dev batch 1\n",
            "epoch: 56 loss train: 1.761324092745781 loss dev: 1.8184966444969177\n",
            "epoch 57 train batch 0\n",
            "epoch 57 train batch 1\n",
            "epoch 57 train batch 2\n",
            "epoch 57 train batch 3\n",
            "epoch 57 train batch 4\n",
            "epoch 57 train batch 5\n",
            "epoch 57 train batch 6\n",
            "epoch 57 train batch 7\n",
            "epoch 57 dev batch 0\n",
            "epoch 57 dev batch 1\n",
            "epoch: 57 loss train: 1.758498728275299 loss dev: 1.8071627616882324\n",
            "epoch 58 train batch 0\n",
            "epoch 58 train batch 1\n",
            "epoch 58 train batch 2\n",
            "epoch 58 train batch 3\n",
            "epoch 58 train batch 4\n",
            "epoch 58 train batch 5\n",
            "epoch 58 train batch 6\n",
            "epoch 58 train batch 7\n",
            "epoch 58 dev batch 0\n",
            "epoch 58 dev batch 1\n",
            "epoch: 58 loss train: 1.750622645020485 loss dev: 1.8018065094947815\n",
            "epoch 59 train batch 0\n",
            "epoch 59 train batch 1\n",
            "epoch 59 train batch 2\n",
            "epoch 59 train batch 3\n",
            "epoch 59 train batch 4\n",
            "epoch 59 train batch 5\n",
            "epoch 59 train batch 6\n",
            "epoch 59 train batch 7\n",
            "epoch 59 dev batch 0\n",
            "epoch 59 dev batch 1\n",
            "epoch: 59 loss train: 1.7518573701381683 loss dev: 1.801614761352539\n",
            "epoch 60 train batch 0\n",
            "epoch 60 train batch 1\n",
            "epoch 60 train batch 2\n",
            "epoch 60 train batch 3\n",
            "epoch 60 train batch 4\n",
            "epoch 60 train batch 5\n",
            "epoch 60 train batch 6\n",
            "epoch 60 train batch 7\n",
            "epoch 60 dev batch 0\n",
            "epoch 60 dev batch 1\n",
            "epoch: 60 loss train: 1.7523051798343658 loss dev: 1.8138505816459656\n",
            "epoch 61 train batch 0\n",
            "epoch 61 train batch 1\n",
            "epoch 61 train batch 2\n",
            "epoch 61 train batch 3\n",
            "epoch 61 train batch 4\n",
            "epoch 61 train batch 5\n",
            "epoch 61 train batch 6\n",
            "epoch 61 train batch 7\n",
            "epoch 61 dev batch 0\n",
            "epoch 61 dev batch 1\n",
            "epoch: 61 loss train: 1.7475449442863464 loss dev: 1.8043007850646973\n",
            "epoch 62 train batch 0\n",
            "epoch 62 train batch 1\n",
            "epoch 62 train batch 2\n",
            "epoch 62 train batch 3\n",
            "epoch 62 train batch 4\n",
            "epoch 62 train batch 5\n",
            "epoch 62 train batch 6\n",
            "epoch 62 train batch 7\n",
            "epoch 62 dev batch 0\n",
            "epoch 62 dev batch 1\n",
            "epoch: 62 loss train: 1.7459932267665863 loss dev: 1.7841484546661377\n",
            "epoch 63 train batch 0\n",
            "epoch 63 train batch 1\n",
            "epoch 63 train batch 2\n",
            "epoch 63 train batch 3\n",
            "epoch 63 train batch 4\n",
            "epoch 63 train batch 5\n",
            "epoch 63 train batch 6\n",
            "epoch 63 train batch 7\n",
            "epoch 63 dev batch 0\n",
            "epoch 63 dev batch 1\n",
            "epoch: 63 loss train: 1.7420362383127213 loss dev: 1.7974363565444946\n",
            "epoch 64 train batch 0\n",
            "epoch 64 train batch 1\n",
            "epoch 64 train batch 2\n",
            "epoch 64 train batch 3\n",
            "epoch 64 train batch 4\n",
            "epoch 64 train batch 5\n",
            "epoch 64 train batch 6\n",
            "epoch 64 train batch 7\n",
            "epoch 64 dev batch 0\n",
            "epoch 64 dev batch 1\n",
            "epoch: 64 loss train: 1.7447900474071503 loss dev: 1.7892535328865051\n",
            "epoch 65 train batch 0\n",
            "epoch 65 train batch 1\n",
            "epoch 65 train batch 2\n",
            "epoch 65 train batch 3\n",
            "epoch 65 train batch 4\n",
            "epoch 65 train batch 5\n",
            "epoch 65 train batch 6\n",
            "epoch 65 train batch 7\n",
            "epoch 65 dev batch 0\n",
            "epoch 65 dev batch 1\n",
            "epoch: 65 loss train: 1.7407126873731613 loss dev: 1.7803194522857666\n",
            "epoch 66 train batch 0\n",
            "epoch 66 train batch 1\n",
            "epoch 66 train batch 2\n",
            "epoch 66 train batch 3\n",
            "epoch 66 train batch 4\n",
            "epoch 66 train batch 5\n",
            "epoch 66 train batch 6\n",
            "epoch 66 train batch 7\n",
            "epoch 66 dev batch 0\n",
            "epoch 66 dev batch 1\n",
            "epoch: 66 loss train: 1.7368762493133545 loss dev: 1.7789198160171509\n",
            "epoch 67 train batch 0\n",
            "epoch 67 train batch 1\n",
            "epoch 67 train batch 2\n",
            "epoch 67 train batch 3\n",
            "epoch 67 train batch 4\n",
            "epoch 67 train batch 5\n",
            "epoch 67 train batch 6\n",
            "epoch 67 train batch 7\n",
            "epoch 67 dev batch 0\n",
            "epoch 67 dev batch 1\n",
            "epoch: 67 loss train: 1.7345995604991913 loss dev: 1.79171884059906\n",
            "epoch 68 train batch 0\n",
            "epoch 68 train batch 1\n",
            "epoch 68 train batch 2\n",
            "epoch 68 train batch 3\n",
            "epoch 68 train batch 4\n",
            "epoch 68 train batch 5\n",
            "epoch 68 train batch 6\n",
            "epoch 68 train batch 7\n",
            "epoch 68 dev batch 0\n",
            "epoch 68 dev batch 1\n",
            "epoch: 68 loss train: 1.7339134216308594 loss dev: 1.7835328578948975\n",
            "epoch 69 train batch 0\n",
            "epoch 69 train batch 1\n",
            "epoch 69 train batch 2\n",
            "epoch 69 train batch 3\n",
            "epoch 69 train batch 4\n",
            "epoch 69 train batch 5\n",
            "epoch 69 train batch 6\n",
            "epoch 69 train batch 7\n",
            "epoch 69 dev batch 0\n",
            "epoch 69 dev batch 1\n",
            "epoch: 69 loss train: 1.732532024383545 loss dev: 1.810674011707306\n",
            "epoch 70 train batch 0\n",
            "epoch 70 train batch 1\n",
            "epoch 70 train batch 2\n",
            "epoch 70 train batch 3\n",
            "epoch 70 train batch 4\n",
            "epoch 70 train batch 5\n",
            "epoch 70 train batch 6\n",
            "epoch 70 train batch 7\n",
            "epoch 70 dev batch 0\n",
            "epoch 70 dev batch 1\n",
            "epoch: 70 loss train: 1.7330083549022675 loss dev: 1.7844622731208801\n",
            "epoch 71 train batch 0\n",
            "epoch 71 train batch 1\n",
            "epoch 71 train batch 2\n",
            "epoch 71 train batch 3\n",
            "epoch 71 train batch 4\n",
            "epoch 71 train batch 5\n",
            "epoch 71 train batch 6\n",
            "epoch 71 train batch 7\n",
            "epoch 71 dev batch 0\n",
            "epoch 71 dev batch 1\n",
            "epoch: 71 loss train: 1.7281838059425354 loss dev: 1.7715279459953308\n",
            "epoch 72 train batch 0\n",
            "epoch 72 train batch 1\n",
            "epoch 72 train batch 2\n",
            "epoch 72 train batch 3\n",
            "epoch 72 train batch 4\n",
            "epoch 72 train batch 5\n",
            "epoch 72 train batch 6\n",
            "epoch 72 train batch 7\n",
            "epoch 72 dev batch 0\n",
            "epoch 72 dev batch 1\n",
            "epoch: 72 loss train: 1.7263937592506409 loss dev: 1.7805668115615845\n",
            "epoch 73 train batch 0\n",
            "epoch 73 train batch 1\n",
            "epoch 73 train batch 2\n",
            "epoch 73 train batch 3\n",
            "epoch 73 train batch 4\n",
            "epoch 73 train batch 5\n",
            "epoch 73 train batch 6\n",
            "epoch 73 train batch 7\n",
            "epoch 73 dev batch 0\n",
            "epoch 73 dev batch 1\n",
            "epoch: 73 loss train: 1.7261427640914917 loss dev: 1.782991886138916\n",
            "epoch 74 train batch 0\n",
            "epoch 74 train batch 1\n",
            "epoch 74 train batch 2\n",
            "epoch 74 train batch 3\n",
            "epoch 74 train batch 4\n",
            "epoch 74 train batch 5\n",
            "epoch 74 train batch 6\n",
            "epoch 74 train batch 7\n",
            "epoch 74 dev batch 0\n",
            "epoch 74 dev batch 1\n",
            "epoch: 74 loss train: 1.7252941578626633 loss dev: 1.769941508769989\n",
            "epoch 75 train batch 0\n",
            "epoch 75 train batch 1\n",
            "epoch 75 train batch 2\n",
            "epoch 75 train batch 3\n",
            "epoch 75 train batch 4\n",
            "epoch 75 train batch 5\n",
            "epoch 75 train batch 6\n",
            "epoch 75 train batch 7\n",
            "epoch 75 dev batch 0\n",
            "epoch 75 dev batch 1\n",
            "epoch: 75 loss train: 1.7220324128866196 loss dev: 1.771532416343689\n",
            "epoch 76 train batch 0\n",
            "epoch 76 train batch 1\n",
            "epoch 76 train batch 2\n",
            "epoch 76 train batch 3\n",
            "epoch 76 train batch 4\n",
            "epoch 76 train batch 5\n",
            "epoch 76 train batch 6\n",
            "epoch 76 train batch 7\n",
            "epoch 76 dev batch 0\n",
            "epoch 76 dev batch 1\n",
            "epoch: 76 loss train: 1.7174490988254547 loss dev: 1.7805389761924744\n",
            "epoch 77 train batch 0\n",
            "epoch 77 train batch 1\n",
            "epoch 77 train batch 2\n",
            "epoch 77 train batch 3\n",
            "epoch 77 train batch 4\n",
            "epoch 77 train batch 5\n",
            "epoch 77 train batch 6\n",
            "epoch 77 train batch 7\n",
            "epoch 77 dev batch 0\n",
            "epoch 77 dev batch 1\n",
            "epoch: 77 loss train: 1.7175295948982239 loss dev: 1.7827557921409607\n",
            "epoch 78 train batch 0\n",
            "epoch 78 train batch 1\n",
            "epoch 78 train batch 2\n",
            "epoch 78 train batch 3\n",
            "epoch 78 train batch 4\n",
            "epoch 78 train batch 5\n",
            "epoch 78 train batch 6\n",
            "epoch 78 train batch 7\n",
            "epoch 78 dev batch 0\n",
            "epoch 78 dev batch 1\n",
            "epoch: 78 loss train: 1.7172900289297104 loss dev: 1.7911324501037598\n",
            "epoch 79 train batch 0\n",
            "epoch 79 train batch 1\n",
            "epoch 79 train batch 2\n",
            "epoch 79 train batch 3\n",
            "epoch 79 train batch 4\n",
            "epoch 79 train batch 5\n",
            "epoch 79 train batch 6\n",
            "epoch 79 train batch 7\n",
            "epoch 79 dev batch 0\n",
            "epoch 79 dev batch 1\n",
            "epoch: 79 loss train: 1.7218377590179443 loss dev: 1.7621504664421082\n",
            "epoch 80 train batch 0\n",
            "epoch 80 train batch 1\n",
            "epoch 80 train batch 2\n",
            "epoch 80 train batch 3\n",
            "epoch 80 train batch 4\n",
            "epoch 80 train batch 5\n",
            "epoch 80 train batch 6\n",
            "epoch 80 train batch 7\n",
            "epoch 80 dev batch 0\n",
            "epoch 80 dev batch 1\n",
            "epoch: 80 loss train: 1.7167010605335236 loss dev: 1.7652058601379395\n",
            "epoch 81 train batch 0\n",
            "epoch 81 train batch 1\n",
            "epoch 81 train batch 2\n",
            "epoch 81 train batch 3\n",
            "epoch 81 train batch 4\n",
            "epoch 81 train batch 5\n",
            "epoch 81 train batch 6\n",
            "epoch 81 train batch 7\n",
            "epoch 81 dev batch 0\n",
            "epoch 81 dev batch 1\n",
            "epoch: 81 loss train: 1.7147747427225113 loss dev: 1.775395393371582\n",
            "epoch 82 train batch 0\n",
            "epoch 82 train batch 1\n",
            "epoch 82 train batch 2\n",
            "epoch 82 train batch 3\n",
            "epoch 82 train batch 4\n",
            "epoch 82 train batch 5\n",
            "epoch 82 train batch 6\n",
            "epoch 82 train batch 7\n",
            "epoch 82 dev batch 0\n",
            "epoch 82 dev batch 1\n",
            "epoch: 82 loss train: 1.7139278501272202 loss dev: 1.7779081463813782\n",
            "epoch 83 train batch 0\n",
            "epoch 83 train batch 1\n",
            "epoch 83 train batch 2\n",
            "epoch 83 train batch 3\n",
            "epoch 83 train batch 4\n",
            "epoch 83 train batch 5\n",
            "epoch 83 train batch 6\n",
            "epoch 83 train batch 7\n",
            "epoch 83 dev batch 0\n",
            "epoch 83 dev batch 1\n",
            "epoch: 83 loss train: 1.712820127606392 loss dev: 1.7756338715553284\n",
            "epoch 84 train batch 0\n",
            "epoch 84 train batch 1\n",
            "epoch 84 train batch 2\n",
            "epoch 84 train batch 3\n",
            "epoch 84 train batch 4\n",
            "epoch 84 train batch 5\n",
            "epoch 84 train batch 6\n",
            "epoch 84 train batch 7\n",
            "epoch 84 dev batch 0\n",
            "epoch 84 dev batch 1\n",
            "epoch: 84 loss train: 1.7095260173082352 loss dev: 1.7763962745666504\n",
            "epoch 85 train batch 0\n",
            "epoch 85 train batch 1\n",
            "epoch 85 train batch 2\n",
            "epoch 85 train batch 3\n",
            "epoch 85 train batch 4\n",
            "epoch 85 train batch 5\n",
            "epoch 85 train batch 6\n",
            "epoch 85 train batch 7\n",
            "epoch 85 dev batch 0\n",
            "epoch 85 dev batch 1\n",
            "epoch: 85 loss train: 1.7084005028009415 loss dev: 1.7649681568145752\n",
            "epoch 86 train batch 0\n",
            "epoch 86 train batch 1\n",
            "epoch 86 train batch 2\n",
            "epoch 86 train batch 3\n",
            "epoch 86 train batch 4\n",
            "epoch 86 train batch 5\n",
            "epoch 86 train batch 6\n",
            "epoch 86 train batch 7\n",
            "epoch 86 dev batch 0\n",
            "epoch 86 dev batch 1\n",
            "epoch: 86 loss train: 1.7054558247327805 loss dev: 1.7654926180839539\n",
            "epoch 87 train batch 0\n",
            "epoch 87 train batch 1\n",
            "epoch 87 train batch 2\n",
            "epoch 87 train batch 3\n",
            "epoch 87 train batch 4\n",
            "epoch 87 train batch 5\n",
            "epoch 87 train batch 6\n",
            "epoch 87 train batch 7\n",
            "epoch 87 dev batch 0\n",
            "epoch 87 dev batch 1\n",
            "epoch: 87 loss train: 1.7044869512319565 loss dev: 1.7585476636886597\n",
            "epoch 88 train batch 0\n",
            "epoch 88 train batch 1\n",
            "epoch 88 train batch 2\n",
            "epoch 88 train batch 3\n",
            "epoch 88 train batch 4\n",
            "epoch 88 train batch 5\n",
            "epoch 88 train batch 6\n",
            "epoch 88 train batch 7\n",
            "epoch 88 dev batch 0\n",
            "epoch 88 dev batch 1\n",
            "epoch: 88 loss train: 1.7005012035369873 loss dev: 1.7728949189186096\n",
            "epoch 89 train batch 0\n",
            "epoch 89 train batch 1\n",
            "epoch 89 train batch 2\n",
            "epoch 89 train batch 3\n",
            "epoch 89 train batch 4\n",
            "epoch 89 train batch 5\n",
            "epoch 89 train batch 6\n",
            "epoch 89 train batch 7\n",
            "epoch 89 dev batch 0\n",
            "epoch 89 dev batch 1\n",
            "epoch: 89 loss train: 1.7024049162864685 loss dev: 1.794434666633606\n",
            "epoch 90 train batch 0\n",
            "epoch 90 train batch 1\n",
            "epoch 90 train batch 2\n",
            "epoch 90 train batch 3\n",
            "epoch 90 train batch 4\n",
            "epoch 90 train batch 5\n",
            "epoch 90 train batch 6\n",
            "epoch 90 train batch 7\n",
            "epoch 90 dev batch 0\n",
            "epoch 90 dev batch 1\n",
            "epoch: 90 loss train: 1.697301521897316 loss dev: 1.767285168170929\n",
            "epoch 91 train batch 0\n",
            "epoch 91 train batch 1\n",
            "epoch 91 train batch 2\n",
            "epoch 91 train batch 3\n",
            "epoch 91 train batch 4\n",
            "epoch 91 train batch 5\n",
            "epoch 91 train batch 6\n",
            "epoch 91 train batch 7\n",
            "epoch 91 dev batch 0\n",
            "epoch 91 dev batch 1\n",
            "epoch: 91 loss train: 1.6975000649690628 loss dev: 1.7716028094291687\n",
            "epoch 92 train batch 0\n",
            "epoch 92 train batch 1\n",
            "epoch 92 train batch 2\n",
            "epoch 92 train batch 3\n",
            "epoch 92 train batch 4\n",
            "epoch 92 train batch 5\n",
            "epoch 92 train batch 6\n",
            "epoch 92 train batch 7\n",
            "epoch 92 dev batch 0\n",
            "epoch 92 dev batch 1\n",
            "epoch: 92 loss train: 1.698989376425743 loss dev: 1.7566904425621033\n",
            "epoch 93 train batch 0\n",
            "epoch 93 train batch 1\n",
            "epoch 93 train batch 2\n",
            "epoch 93 train batch 3\n",
            "epoch 93 train batch 4\n",
            "epoch 93 train batch 5\n",
            "epoch 93 train batch 6\n",
            "epoch 93 train batch 7\n",
            "epoch 93 dev batch 0\n",
            "epoch 93 dev batch 1\n",
            "epoch: 93 loss train: 1.6968429386615753 loss dev: 1.7623211145401\n",
            "epoch 94 train batch 0\n",
            "epoch 94 train batch 1\n",
            "epoch 94 train batch 2\n",
            "epoch 94 train batch 3\n",
            "epoch 94 train batch 4\n",
            "epoch 94 train batch 5\n",
            "epoch 94 train batch 6\n",
            "epoch 94 train batch 7\n",
            "epoch 94 dev batch 0\n",
            "epoch 94 dev batch 1\n",
            "epoch: 94 loss train: 1.6948246210813522 loss dev: 1.752945899963379\n",
            "epoch 95 train batch 0\n",
            "epoch 95 train batch 1\n",
            "epoch 95 train batch 2\n",
            "epoch 95 train batch 3\n",
            "epoch 95 train batch 4\n",
            "epoch 95 train batch 5\n",
            "epoch 95 train batch 6\n",
            "epoch 95 train batch 7\n",
            "epoch 95 dev batch 0\n",
            "epoch 95 dev batch 1\n",
            "epoch: 95 loss train: 1.6906167417764664 loss dev: 1.755047857761383\n",
            "epoch 96 train batch 0\n",
            "epoch 96 train batch 1\n",
            "epoch 96 train batch 2\n",
            "epoch 96 train batch 3\n",
            "epoch 96 train batch 4\n",
            "epoch 96 train batch 5\n",
            "epoch 96 train batch 6\n",
            "epoch 96 train batch 7\n",
            "epoch 96 dev batch 0\n",
            "epoch 96 dev batch 1\n",
            "epoch: 96 loss train: 1.690156877040863 loss dev: 1.7937846779823303\n",
            "epoch 97 train batch 0\n",
            "epoch 97 train batch 1\n",
            "epoch 97 train batch 2\n",
            "epoch 97 train batch 3\n",
            "epoch 97 train batch 4\n",
            "epoch 97 train batch 5\n",
            "epoch 97 train batch 6\n",
            "epoch 97 train batch 7\n",
            "epoch 97 dev batch 0\n",
            "epoch 97 dev batch 1\n",
            "epoch: 97 loss train: 1.6910737454891205 loss dev: 1.7548965811729431\n",
            "epoch 98 train batch 0\n",
            "epoch 98 train batch 1\n",
            "epoch 98 train batch 2\n",
            "epoch 98 train batch 3\n",
            "epoch 98 train batch 4\n",
            "epoch 98 train batch 5\n",
            "epoch 98 train batch 6\n",
            "epoch 98 train batch 7\n",
            "epoch 98 dev batch 0\n",
            "epoch 98 dev batch 1\n",
            "epoch: 98 loss train: 1.689713567495346 loss dev: 1.7528563737869263\n",
            "epoch 99 train batch 0\n",
            "epoch 99 train batch 1\n",
            "epoch 99 train batch 2\n",
            "epoch 99 train batch 3\n",
            "epoch 99 train batch 4\n",
            "epoch 99 train batch 5\n",
            "epoch 99 train batch 6\n",
            "epoch 99 train batch 7\n",
            "epoch 99 dev batch 0\n",
            "epoch 99 dev batch 1\n",
            "epoch: 99 loss train: 1.6886399388313293 loss dev: 1.7732487916946411\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpJKVI767RHn",
        "colab_type": "code",
        "outputId": "00c62bec-1798-4552-c681-023def65b0ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "#Plot loss\n",
        "plt.figure(1)\n",
        "plt.plot(range(len(loss_history_train)),loss_history_train)\n",
        "plt.plot(range(len(loss_history_dev)),loss_history_dev)\n",
        "plt.title('Loss over epoch')\n",
        "plt.legend(['Train loss','Dev loss'])\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRBAR0bgkgoiIxiURREQ07m/3a/kYZyuYwgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6PQX-IA5Pu7",
        "colab_type": "text"
      },
      "source": [
        "#Test\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "594COXSX5MEX",
        "colab_type": "code",
        "outputId": "78beaf8f-4e22-40d6-a90f-95b49369d462",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "#Test set\n",
        "loss_history_test = []\n",
        "loss_total_test = 0\n",
        "class_correct = np.zeros(len(classes))\n",
        "class_total = np.zeros(len(classes))\n",
        "\n",
        "model.eval()\n",
        "j = 0\n",
        "for input, target in test_loader:\n",
        "    print('test batch',j)\n",
        "    if cuda_enabled:\n",
        "        input, target = input.cuda(), target.cuda()\n",
        "    # Forward Propagation\n",
        "    output = model(input)\n",
        "    # Compute and print loss\n",
        "    loss_test = loss_function(output, target)\n",
        "    loss_total_test += loss_test.item()\n",
        "    # convert output probabilities to predicted class\n",
        "    _, prediction = torch.max(output,1)\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = prediction.eq(target.data.view_as(prediction))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not cuda_enabled else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    # calculate test accuracy for each object class\n",
        "    for k in range(batch_size):\n",
        "        label = target.data[k]\n",
        "        class_correct[label] += correct[k].item()\n",
        "        class_total[label] += 1\n",
        "    j +=1\n",
        "    \n",
        "# Test loss\n",
        "lose_avg_test = float(loss_total_test)/j \n",
        "loss_history_test.append(lose_avg_test)\n",
        "print('loss test:', lose_avg_test)\n",
        "\n",
        "# Accuracy of each class\n",
        "for i in range(10):\n",
        "    print('Test Accuracy of', classes[i],':', \n",
        "          100 * class_correct[i] / class_total[i],'%',\n",
        "          '(',np.sum(class_correct[i]), '/',\n",
        "          np.sum(class_total[i]),')')\n",
        "\n",
        "print('\\nTest Accuracy (Overall):',  \n",
        "    100 * np.sum(class_correct) / np.sum(class_total),'%',\n",
        "    '(', np.sum(class_correct), '/',\n",
        "    np.sum(class_total),')')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test batch 0\n",
            "test batch 1\n",
            "loss test: 1.7765164971351624\n",
            "Test Accuracy of airplane : 71.7 % ( 717.0 / 1000.0 )\n",
            "Test Accuracy of automobile : 79.3 % ( 793.0 / 1000.0 )\n",
            "Test Accuracy of bird : 58.8 % ( 588.0 / 1000.0 )\n",
            "Test Accuracy of cat : 51.0 % ( 510.0 / 1000.0 )\n",
            "Test Accuracy of deer : 62.9 % ( 629.0 / 1000.0 )\n",
            "Test Accuracy of dog : 63.6 % ( 636.0 / 1000.0 )\n",
            "Test Accuracy of frog : 67.9 % ( 679.0 / 1000.0 )\n",
            "Test Accuracy of horse : 74.2 % ( 742.0 / 1000.0 )\n",
            "Test Accuracy of ship : 86.7 % ( 867.0 / 1000.0 )\n",
            "Test Accuracy of truck : 77.4 % ( 774.0 / 1000.0 )\n",
            "\n",
            "Test Accuracy (Overall): 69.35 % ( 6935.0 / 10000.0 )\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}